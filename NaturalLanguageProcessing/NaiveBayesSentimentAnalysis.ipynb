{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being tired given that you ran: 0.6\n",
      "Probability of being tired given that you ran: 0.6\n",
      "Final classification for new day: was tired. Tired probability: 0.10204081632653061. Not tired probability: 0.054421768707482984.\n"
     ]
    }
   ],
   "source": [
    "## 1. Introduction ##\n",
    "\n",
    "# Let's say this is your running history for the past week\n",
    "# For each day, it records whether or not you ran, and whether or not you were tired\n",
    "days = [[\"ran\", \"was tired\"], [\"ran\", \"was not tired\"], [\"didn't run\", \"was tired\"], [\"ran\", \"was tired\"], [\"didn't run\", \"was not tired\"], [\"ran\", \"was not tired\"], [\"ran\", \"was tired\"]]\n",
    "\n",
    "# Let's say we want to use Bayes' theorem to calculate the odds that you were tired, given that you ran\n",
    "# This is P(A)\n",
    "prob_tired = len([d for d in days if d[1] == \"was tired\"]) / len(days)\n",
    "# This is P(B)\n",
    "prob_ran = len([d for d in days if d[0] == \"ran\"]) / len(days)\n",
    "# This is P(B|A)\n",
    "prob_ran_given_tired = len([d for d in days if d[0] == \"ran\" and d[1] == \"was tired\"]) / len([d for d in days if d[1] == \"was tired\"])\n",
    "\n",
    "# Now we can calculate P(A|B)\n",
    "prob_tired_given_ran = (prob_ran_given_tired * prob_tired) / prob_ran\n",
    "\n",
    "print(\"Probability of being tired given that you ran: {0}\".format(prob_tired_given_ran))\n",
    "\n",
    "prob_tired_given_ran_v2 = len([d for d in days if d[0] == \"ran\" and d[1] == \"was tired\"]) / len([d for d in days if d[0] == \"ran\"])\n",
    "\n",
    "print(\"Probability of being tired given that you ran: {0}\".format(prob_tired_given_ran_v2))\n",
    "\n",
    "## 2. Overview of Naive Bayes ##\n",
    "\n",
    "# Here's our data, but with \"woke up early\" or \"didn't wake up early\" added\n",
    "days = [[\"ran\", \"was tired\", \"woke up early\"], [\"ran\", \"was not tired\", \"didn't wake up early\"], [\"didn't run\", \"was tired\", \"woke up early\"], [\"ran\", \"was tired\", \"didn't wake up early\"], [\"didn't run\", \"was tired\", \"woke up early\"], [\"ran\", \"was not tired\", \"didn't wake up early\"], [\"ran\", \"was tired\", \"woke up early\"]]\n",
    "\n",
    "# We're trying to predict whether or not you were tired on this day\n",
    "new_day = [\"ran\", \"didn't wake up early\"]\n",
    "\n",
    "def calc_y_probability(y_label, days):\n",
    "    return len([d for d in days if d[1] == y_label]) / len(days)\n",
    "\n",
    "def calc_ran_probability_given_y(ran_label, y_label, days):\n",
    "    return len([d for d in days if d[1] == y_label and d[0] == ran_label]) / len(days)\n",
    "\n",
    "def calc_woke_early_probability_given_y(woke_label, y_label, days):\n",
    "    return len([d for d in days if d[1] == y_label and d[2] == woke_label]) / len(days)\n",
    "\n",
    "denominator = len([d for d in days if d[0] == new_day[0] and d[2] == new_day[1]]) / len(days)\n",
    "# Plug all the values into our formula\n",
    "# Multiply the class (y) probability, and the probability of the x-values occurring given that class\n",
    "prob_tired = (calc_y_probability(\"was tired\", days) * calc_ran_probability_given_y(new_day[0], \"was tired\", days) * calc_woke_early_probability_given_y(new_day[1], \"was tired\", days)) / denominator\n",
    "\n",
    "prob_not_tired = (calc_y_probability(\"was not tired\", days) * calc_ran_probability_given_y(new_day[0], \"was not tired\", days) * calc_woke_early_probability_given_y(new_day[1], \"was not tired\", days)) / denominator\n",
    "\n",
    "# Make a classification decision based on the probabilities\n",
    "classification = \"was tired\"\n",
    "if prob_not_tired > prob_tired:\n",
    "    classification = \"was not tired\"\n",
    "\n",
    "print(\"Final classification for new day: {0}. Tired probability: {1}. Not tired probability: {2}.\".format(classification, prob_tired, prob_not_tired))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative text sample: plot : two teen couples go to a church party drink and then drive . they get into an accident . one \n",
      "Positive text sample: films adapted from comic books have had plenty of success whether they're about superheroes ( batman\n",
      "Review: plot : two teen couples go to a church party drink and then drive . they get into an accident . one of the guys dies but his girlfriend continues to see him in her life and has nightmares . what's the deal ? watch the movie and \" sorta \" find out . . . critique : a mind-fuck movie for the teen generation that touches on a very cool idea but presents it in a very bad package . which is what makes this review an even harder one to write since i generally applaud films which attempt\n",
      "Negative prediction: 3.005053036235652e-221\n",
      "Positive prediction: 1.307170546690679e-226\n",
      "AUC of the predictions: 0.680701754385965\n"
     ]
    }
   ],
   "source": [
    "## 3. Finding Word Counts ##\n",
    "\n",
    "# A nice Python class that lets us count how many times items occur in a list\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Read in the training data\n",
    "with open(\"movie_reviews_train.csv\", 'r') as file:\n",
    "    reviews = list(csv.reader(file))\n",
    "\n",
    "def get_text(reviews, score):\n",
    "    # Join together the text in the reviews for a particular tone\n",
    "    # Lowercase the text so that the algorithm doesn't see \"Not\" and \"not\" as different words, for example\n",
    "    return \" \".join([r[0].lower() for r in reviews if r[1] == str(score)])\n",
    "\n",
    "def count_text(text):\n",
    "    # Split text into words based on whitespace -- simple but effective\n",
    "    words = re.split(\"\\s+\", text)\n",
    "    # Count up the occurrence of each word\n",
    "    return Counter(words)\n",
    "\n",
    "negative_text = get_text(reviews, -1)\n",
    "positive_text = get_text(reviews, 1)\n",
    "# Generate word counts for negative tone\n",
    "negative_counts = count_text(negative_text)\n",
    "# Generate word counts for positive tone\n",
    "positive_counts = count_text(positive_text)\n",
    "\n",
    "print(\"Negative text sample: {0}\".format(negative_text[:100]))\n",
    "print(\"Positive text sample: {0}\".format(positive_text[:100]))\n",
    "\n",
    "## 4. Making Predictions About Review Classifications ##\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def get_y_count(score):\n",
    "    # Compute the count of each classification occurring in the data\n",
    "    return len([r for r in reviews if r[1] == str(score)])\n",
    "\n",
    "# We'll use these counts for smoothing when computing the prediction\n",
    "positive_review_count = get_y_count(1)\n",
    "negative_review_count = get_y_count(-1)\n",
    "\n",
    "# These are the class probabilities (we saw them in the formula as P(y))\n",
    "prob_positive = positive_review_count / len(reviews)\n",
    "prob_negative = negative_review_count / len(reviews)\n",
    "\n",
    "def make_class_prediction(text, counts, class_prob, class_count):\n",
    "    prediction = 1\n",
    "    text_counts = Counter(re.split(\"\\s+\", text))\n",
    "    for word in text_counts:\n",
    "        # For every word in the text, we get the number of times that word occurred in the reviews for a given class, add 1 to smooth the value, and divide by the total number of words in the class (plus the class_count, also to smooth the denominator)\n",
    "        # Smoothing ensures that we don't multiply the prediction by 0 if the word didn't exist in the training data\n",
    "        # We also smooth the denominator counts to keep things even\n",
    "        prediction *=  text_counts.get(word) * ((counts.get(word, 0) + 1) / (sum(counts.values()) + class_count))\n",
    "    # Now we multiply by the probability of the class existing in the documents\n",
    "    return prediction * class_prob\n",
    "\n",
    "# Now we can generate probabilities for the classes our reviews belong to\n",
    "# The probabilities themselves aren't very useful -- we make our classification decision based on which value is greater\n",
    "print(\"Review: {0}\".format(reviews[0][0]))\n",
    "\n",
    "print(\"Negative prediction: {0}\".format(make_class_prediction(reviews[0][0], negative_counts, prob_negative, negative_review_count)))\n",
    "\n",
    "print(\"Positive prediction: {0}\".format(make_class_prediction(reviews[0][0], positive_counts, prob_positive, positive_review_count)))\n",
    "\n",
    "## 5. Predicting the Test Set ##\n",
    "\n",
    "import csv\n",
    "\n",
    "def make_decision(text, make_class_prediction):\n",
    "    # Compute the negative and positive probabilities\n",
    "    negative_prediction = make_class_prediction(text, negative_counts, prob_negative, negative_review_count)\n",
    "    positive_prediction = make_class_prediction(text, positive_counts, prob_positive, positive_review_count)\n",
    "\n",
    "    # We assign a classification based on which probability is greater\n",
    "    if negative_prediction > positive_prediction:\n",
    "      return -1\n",
    "    return 1\n",
    "\n",
    "with open(\"movie_reviews_test.csv\", 'r') as file:\n",
    "    test = list(csv.reader(file))\n",
    "\n",
    "    \n",
    "predictions = [make_decision(r[0], make_class_prediction) for r in test]\n",
    "\n",
    "## 6. Computing Prediction Error ##\n",
    "\n",
    "actual = [int(r[1]) for r in test]\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Generate the ROC curve using scikits-learn\n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
    "\n",
    "# Measure the area under the curve\n",
    "# The closer to 1 it is, the \"better\" the predictions\n",
    "\n",
    "print(\"AUC of the predictions: {0}\".format(metrics.auc(fpr, tpr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
